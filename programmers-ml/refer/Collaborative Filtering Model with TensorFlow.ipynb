{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The collaborative filter approach focuses on finding users who have given similar ratings to the same books, thus creating a link between users, to whom will be suggested books that were reviewed in a positive way. In this way, we look for associations between users, not between books."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 438647: expected 1 fields, saw 3\\n'\n",
      "b'Skipping line 543300: expected 1 fields, saw 2\\nSkipping line 723138: expected 1 fields, saw 2\\nSkipping line 738099: expected 1 fields, saw 2\\nSkipping line 908731: expected 1 fields, saw 2\\n'\n",
      "b'Skipping line 6451: expected 8 fields, saw 9\\nSkipping line 43666: expected 8 fields, saw 10\\nSkipping line 51750: expected 8 fields, saw 9\\n'\n",
      "b'Skipping line 92037: expected 8 fields, saw 9\\nSkipping line 104318: expected 8 fields, saw 9\\nSkipping line 121767: expected 8 fields, saw 9\\n'\n",
      "b'Skipping line 144057: expected 8 fields, saw 9\\nSkipping line 150788: expected 8 fields, saw 9\\nSkipping line 157127: expected 8 fields, saw 9\\nSkipping line 180188: expected 8 fields, saw 9\\nSkipping line 185737: expected 8 fields, saw 9\\n'\n",
      "b'Skipping line 209387: expected 8 fields, saw 9\\nSkipping line 220625: expected 8 fields, saw 9\\nSkipping line 227932: expected 8 fields, saw 11\\nSkipping line 228956: expected 8 fields, saw 10\\nSkipping line 245932: expected 8 fields, saw 9\\nSkipping line 251295: expected 8 fields, saw 9\\nSkipping line 259940: expected 8 fields, saw 9\\nSkipping line 261528: expected 8 fields, saw 9\\n'\n"
     ]
    }
   ],
   "source": [
    "rating = pd.read_csv('data/BX-Book-Ratings.csv', sep=';', error_bad_lines=False, encoding=\"latin-1\")\n",
    "user = pd.read_csv('data/BX-Users.csv', sep=';', error_bad_lines=False, encoding=\"latin-1\")\n",
    "book = pd.read_csv('data/BX-Books.csv', sep=';', error_bad_lines=False, encoding=\"latin-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>nyc, new york, usa</th>\n",
       "      <th>\\N</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>stockton, california, usa</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>moscow, yukon territory, russia</td>\n",
       "      <td>\\N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>porto, v.n.gaia, portugal</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>farnborough, hants, united kingdom</td>\n",
       "      <td>\\N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>santa monica, california, usa</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278852</th>\n",
       "      <td>278854</td>\n",
       "      <td>portland, oregon, usa</td>\n",
       "      <td>\\N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278853</th>\n",
       "      <td>278855</td>\n",
       "      <td>tacoma, washington, united kingdom</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278854</th>\n",
       "      <td>278856</td>\n",
       "      <td>brampton, ontario, canada</td>\n",
       "      <td>\\N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278855</th>\n",
       "      <td>278857</td>\n",
       "      <td>knoxville, tennessee, usa</td>\n",
       "      <td>\\N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278856</th>\n",
       "      <td>278858</td>\n",
       "      <td>dublin, n/a, ireland</td>\n",
       "      <td>\\N</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>278857 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             1                  nyc, new york, usa  \\N\n",
       "0            2           stockton, california, usa  18\n",
       "1            3     moscow, yukon territory, russia  \\N\n",
       "2            4           porto, v.n.gaia, portugal  17\n",
       "3            5  farnborough, hants, united kingdom  \\N\n",
       "4            6       santa monica, california, usa  61\n",
       "...        ...                                 ...  ..\n",
       "278852  278854               portland, oregon, usa  \\N\n",
       "278853  278855  tacoma, washington, united kingdom  50\n",
       "278854  278856           brampton, ontario, canada  \\N\n",
       "278855  278857           knoxville, tennessee, usa  \\N\n",
       "278856  278858                dublin, n/a, ireland  \\N\n",
       "\n",
       "[278857 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_rating = pd.merge(rating, book, on='ISBN')\n",
    "cols = ['Year-Of-Publication', 'Publisher', 'Book-Author', 'Image-URL-S', 'Image-URL-M', 'Image-URL-L']\n",
    "book_rating.drop(cols, axis=1, inplace=True)\n",
    "book_rating.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_rating.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_count = (book_rating.\n",
    "     groupby(by = ['Book-Title'])['Book-Rating'].\n",
    "     count().\n",
    "     reset_index().\n",
    "     rename(columns = {'Book-Rating': 'RatingCount_book'})\n",
    "     [['Book-Title', 'RatingCount_book']]\n",
    "    )\n",
    "rating_count.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 25\n",
    "rating_count = rating_count.query('RatingCount_book >= @threshold')\n",
    "rating_count.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_rating.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_rating = pd.merge(rating_count, book_rating, left_on='Book-Title', right_on='Book-Title', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_rating.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_count = (user_rating.\n",
    "     groupby(by = ['User-ID'])['Book-Rating'].\n",
    "     count().\n",
    "     reset_index().\n",
    "     rename(columns = {'Book-Rating': 'RatingCount_user'})\n",
    "     [['User-ID', 'RatingCount_user']]\n",
    "    )\n",
    "user_count.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 20\n",
    "user_count = user_count.query('RatingCount_user >= @threshold')\n",
    "user_count.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = user_rating.merge(user_count, left_on = 'User-ID', right_on = 'User-ID', how = 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of unique books: ', combined['Book-Title'].nunique())\n",
    "print('Number of unique users: ', combined['User-ID'].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize the ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "combined['Book-Rating'] = combined['Book-Rating'].values.astype(float)\n",
    "rating_scaled = pd.DataFrame(scaler.fit_transform(combined['Book-Rating'].values.reshape(-1,1)))\n",
    "combined['Book-Rating'] = rating_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abd build the user book matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = combined.drop_duplicates(['User-ID', 'Book-Title'])\n",
    "user_book_matrix = combined.pivot(index='User-ID', columns='Book-Title', values='Book-Rating')\n",
    "user_book_matrix.fillna(0, inplace=True)\n",
    "\n",
    "users = user_book_matrix.index.tolist()\n",
    "books = user_book_matrix.columns.tolist()\n",
    "\n",
    "user_book_matrix = user_book_matrix.as_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf.placeholder only available in v1, so we have to work around. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will initialize the TensorFlow placeholder. Then, weights and biases are randomly initialized, the following code are taken from the book: Python Machine Learning Cook Book - Second Edition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_input = combined['Book-Title'].nunique()\n",
    "num_hidden_1 = 10\n",
    "num_hidden_2 = 5\n",
    "\n",
    "X = tf.placeholder(tf.float64, [None, num_input])\n",
    "\n",
    "weights = {\n",
    "    'encoder_h1': tf.Variable(tf.random_normal([num_input, num_hidden_1], dtype=tf.float64)),\n",
    "    'encoder_h2': tf.Variable(tf.random_normal([num_hidden_1, num_hidden_2], dtype=tf.float64)),\n",
    "    'decoder_h1': tf.Variable(tf.random_normal([num_hidden_2, num_hidden_1], dtype=tf.float64)),\n",
    "    'decoder_h2': tf.Variable(tf.random_normal([num_hidden_1, num_input], dtype=tf.float64)),\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'encoder_b1': tf.Variable(tf.random_normal([num_hidden_1], dtype=tf.float64)),\n",
    "    'encoder_b2': tf.Variable(tf.random_normal([num_hidden_2], dtype=tf.float64)),\n",
    "    'decoder_b1': tf.Variable(tf.random_normal([num_hidden_1], dtype=tf.float64)),\n",
    "    'decoder_b2': tf.Variable(tf.random_normal([num_input], dtype=tf.float64)),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can build the encoder and decoder model, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder(x):\n",
    "    layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(x, weights['encoder_h1']), biases['encoder_b1']))\n",
    "    layer_2 = tf.nn.sigmoid(tf.add(tf.matmul(layer_1, weights['encoder_h2']), biases['encoder_b2']))\n",
    "    return layer_2\n",
    "\n",
    "def decoder(x):\n",
    "    layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(x, weights['decoder_h1']), biases['decoder_b1']))\n",
    "    layer_2 = tf.nn.sigmoid(tf.add(tf.matmul(layer_1, weights['decoder_h2']), biases['decoder_b2']))\n",
    "    return layer_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will construct the model and the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_op = encoder(X)\n",
    "decoder_op = decoder(encoder_op)\n",
    "\n",
    "y_pred = decoder_op\n",
    "\n",
    "y_true = X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define loss function and optimizer, and minimize the squared error, and define the evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.losses.mean_squared_error(y_true, y_pred)\n",
    "optimizer = tf.train.RMSPropOptimizer(0.03).minimize(loss)\n",
    "eval_x = tf.placeholder(tf.int32, )\n",
    "eval_y = tf.placeholder(tf.int32, )\n",
    "pre, pre_op = tf.metrics.precision(labels=eval_x, predictions=eval_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the variables. Because TensorFlow uses computational graphs for its operations, placeholders and variables must be initialized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "local_init = tf.local_variables_initializer()\n",
    "pred_data = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can finally start to train our model.\n",
    "\n",
    "We split training data into batches, and we feed the network with them.\n",
    "\n",
    "We train our model with vectors of user ratings, each vector represents a user and each column a book, and entries are ratings that the user gave to books. \n",
    "\n",
    "After a few trials, I discovered that training model for 5 epochs with a batch size of 10 would be consum enough memory. This means that the entire training set will feed our neural network 20 times, every time using 50 users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with tf.Session() as session:\n",
    "    epochs = 100\n",
    "    batch_size = 35\n",
    "\n",
    "    session.run(init)\n",
    "    session.run(local_init)\n",
    "\n",
    "    num_batches = int(user_book_matrix.shape[0] / batch_size)\n",
    "    user_book_matrix = np.array_split(user_book_matrix, num_batches)\n",
    "    \n",
    "    for i in range(epochs):\n",
    "\n",
    "        avg_cost = 0\n",
    "        for batch in user_book_matrix:\n",
    "            _, l = session.run([optimizer, loss], feed_dict={X: batch})\n",
    "            avg_cost += l\n",
    "\n",
    "        avg_cost /= num_batches\n",
    "\n",
    "        print(\"epoch: {} Loss: {}\".format(i + 1, avg_cost))\n",
    "\n",
    "    user_book_matrix = np.concatenate(user_book_matrix, axis=0)\n",
    "\n",
    "    preds = session.run(decoder_op, feed_dict={X: user_book_matrix})\n",
    "\n",
    "    pred_data = pred_data.append(pd.DataFrame(preds))\n",
    "\n",
    "    pred_data = pred_data.stack().reset_index(name='Book-Rating')\n",
    "    pred_data.columns = ['User-ID', 'Book-Title', 'Book-Rating']\n",
    "    pred_data['User-ID'] = pred_data['User-ID'].map(lambda value: users[value])\n",
    "    pred_data['Book-Title'] = pred_data['Book-Title'].map(lambda value: books[value])\n",
    "    \n",
    "    keys = ['User-ID', 'Book-Title']\n",
    "    index_1 = pred_data.set_index(keys).index\n",
    "    index_2 = combined.set_index(keys).index\n",
    "\n",
    "    top_ten_ranked = pred_data[~index_1.isin(index_2)]\n",
    "    top_ten_ranked = top_ten_ranked.sort_values(['User-ID', 'Book-Rating'], ascending=[True, False])\n",
    "    top_ten_ranked = top_ten_ranked.groupby('User-ID').head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_ten_ranked.loc[top_ten_ranked['User-ID'] == 278582]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_rating.loc[book_rating['User-ID'] == 278582].sort_values(by=['Book-Rating'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
